---
title:  'Data Analysis Project: Knee Replacement Surgery Outcomes'
subtitle: 'Analysis Project'
date: "December 14, 2016"
output:
  html_document:
    toc: yes
author:
- Gitika Jain (gitikaj2)
- Heather Sewak (hsewak2)
- Ionel Miu (imiu2)
- Jared Colbert (jaredc2)
---
#  Knee Replacement Surgery Outcome

## Introduction
This research aims to determine if the postoperative function of a total knee replacement recipient can be modeled based on anthropomorphic data, and details of the surgical procedure. We decided to carry out testing on models with two postoperative scoring systems as the response variable.  The first score is the "Surgeon's Score" which measures more objective postoperative metrics about the condition of the replaced knee following surgery.  These metrics include: how straight or how flexed the patient can make their knee, how well aligned the knee appears when inspecting from a frontal perspective, and how much laxity or separation can be achieved by placing the lower leg under tension.  Finally, the patient is asked about the overall pain they experience with the knee.  The resulting score is a value between 0-100 with 80-100 being an excellent result.

The second score we modeled is the "Patient's Score" which is generated by three questions: how far are they able to walk, how well can they navigate stairs, and do they require any walking aids (crutches, cane, etc).

We hope to create a model that can predict each of the scores based on a variety of categorical and numeric predictors. 

## Background information
This data has been collected from a Southern California Hospital in collaboration with the Shiley Center for Orthopedic Research.  The data is used for both clinical research and postoperative monitoring of implant survivorship.  All patient specific information has been removed from the data so it adheres to the Health Insurance Portability and Accountability Act (HIPAA).

## Statement
We hope to determine the impact a particular surgeon or the type of implants used has on the patient's outcome after surgery.  We are also interested in exploring the potential relationship between anthropomorphic factors and the patient's postoperative satisfaction.  Finally, we would like to understand if the considerable expense of computer navigation in knee replacement leads to better postoperative scores. Nevertheless, we keep our options opened to investigate other aspects that were not 'visible' or thought about but might reveal themselves as we move on with the project.


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lmtest)
library(car)
purple = '#772299'; green = '#3FE500'; red = '#E50000'; lightblue = '#229999'; blue = '#336699'
```

```{r}
set.seed(19920429)
knee = read.csv('knees.csv')
knee$Surgeon = as.factor(knee$Surgeon)
knee$Year = as.factor(knee$Year)
```

### Functions for Model Evaluation

```{r}
#Function to plot various plots as QQPlot, Fitted vs Residual Plot.
showPlots = function(model) {
  par(mfcol=c(1,3))
  qqnorm(resid(model), col = red, pch = 16)
  qqline(resid(model), col = lightblue, lwd = 2)
  plot(fitted(model), resid(model), xlab = 'Fitted Values', ylab = 'Residuals', main = 'Fitted vs. Residuals Plot', col = purple, pch = 16)
  abline(h = 0, col = green, lwd = 2)
  hist(resid(model), col = blue, main = 'Histogram of Residuals', xlab = 'Residuals')
}

#Function to calculate various statistics which we use to evaluate the model
evaluate = function(model){
  # Run each of the tests
  shapiroTest = shapiro.test(resid(model))
  bpTest = bptest(model)
  rSquared = summary(model)$r.squared
  adjRSquared = summary(model)$adj.r.squared
  loocv = sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
  large_hat = sum(hatvalues(model) > 2 * mean(hatvalues(model)))
  large_resid = sum(rstandard(model)[abs(rstandard(model)) > 2]) / length(rstandard(model))
  large_cooks = sum(cooks.distance(model) > 4 / length(cooks.distance(model)))
  
  # Collect tests in dataframe
  values = data.frame(Result = c(prettyNum(shapiroTest$p.value), prettyNum(bpTest$p.value), prettyNum(rSquared), prettyNum(adjRSquared),prettyNum(loocv), prettyNum(large_hat), prettyNum(large_cooks))) 
  rowNames = data.frame(Test = c('Shapiro Wilk pvalue', 'Breusch-Pagan pvalue', 'R Squared', 'Adj R Squared', 'Cross Validation', 'Large Hat Values', 'Influential'))

  summary_output = cbind(rowNames, values)
  show(summary_output)
}
# find and remove all influential values and return the resulting dataframe
removeInfluential = function(model, data){
  cooks = which(cooks.distance(model) > 4 / length(cooks.distance(model)))
  newData = data[-cooks , ]
  return(newData)
}

# identify all influential data points
IdentifyInfluential = function(model, data){
  cooks = which(cooks.distance(model) > 4 / length(cooks.distance(model)))
  return(index_)
}

# find large hat values, subtract them from the dataframe and return the dataframe
removeLargeHatValues = function(model, data){
  largeHat = which(hatvalues(model) > 2 * mean(hatvalues(model)))
  newData = data[-largeHat, ]
  return(newData)
}

# calculate RMSE
rmse  = function(actual, predicted) {  sqrt(mean((actual - predicted) ^ 2))}

```

### Synopsis of Workflow

#### Outline Questions
After we had obtained a copy of the current patient database we spent some time inspecting each of the columns to ensure that each member of the team had a consistent understanding of what each datapoint measures and how it is collected.  We then crafted a series of questions we could ask and speculated on how each of the datapoints might be organized in various models.  It was apparent to us that the response variables would have to be the Surgeon or Patient scores since each of our questions sought to explain factors that determine a patient's satisfaction after surgery.  The two scores are the only concrete datapoint with which we could measure a post-surgical outcome.

#### Data Preparation
The original dataset was full of errant or missing fields which required cleaning.  Because the complete dataset is quite large, we were able to drop all rows with missing or NA values while retaining nearly two thousand entries.  We then used some visualization techniques to get a 'feel' for which items may be appropriate to utilize in a model.  Finally, the data was separated into training and testing groups.

#### Initial Model
We then began model creation starting with a full additive model.  Upon evaluation we found the model to be a poor fit with metrics not satisfying any of our criteria for a reasonable model.  While testing for leverages we spotted some errant entries that remained in the dataset so a second cleaning pass through the data was required.

#### Selective, AIC, BIC and Interactive Models
Next we checked for variance between the full model and all iterations of models with a single predictor removed.  This process resulted in a collection of six significan predictors which were then utilized in fitting a smaller model.  AIC and BIC selection was performed also performed and the assumptions were tested after each model was created.  Since no antecedent model performed as hoped, we turned to a series of interactive models.  We utilized the anova function extensively to determine if the variance between models was significant.

#### Removing Leverage
We then hypothesized that the outliers in the data might be hindering our ability to fit a model with a sufficiently hight Adjusted $R^2$.  The values with a high Cook's Distance were identified and removed from subsequent models.

#### Evaluation with RMSE
Finally, we evaluated each of the fitted models on test data unseen by any model.  The Root Mean Square Error was the final metric utilized to determine the quality of each model.  Based upon the normality of the residuals, Adjusted R-Squared, Cross Validation and RMSE we selected a single model as the best model.

### Knee Score Surgeon

Before moving onto the formal analysis of our data set, we started by performing visual inspection by plotting dependencies among our the variables.
The main focus was to look for any visual relationship between `KneeScore_Surgeon, KneeScore_Patient` and the rest of the variables. 
Our choice was to use `scatterplotMatrix` from the `car` package as it plots regression lines along with the actual data.

```{r, fig.height=10, fig.width=12, warning=FALSE}
scatterplotMatrix(~ KneeScore_Surgeon + KneeScore_Patient + Surgeon + Year + Age + Gender + Weight + Height, span=0.7, data=knee)
```

**Comments:** Based on the above graphics, `KneeScore_Surgeon, KneeScore_Patient` seems to behave similarly across all variables. This is expected as both, surgeon and patient, should be in consensus in regards to the success or quality of a procedure.

For a better view of the above statement, we plotted the following plot, showing a strong relationship between `KneeScore_Surgeon` and `KneeScore_Patient`.

```{r, fig.height=10, fig.width=12, warning=FALSE}
scatterplotMatrix(~ KneeScore_Surgeon + KneeScore_Patient, span=0.7, data=knee)
```

The distribution of `KneeScore_Surgeon` which reflects the score of a procedure assigned be the `Surgeon` is unimodal distribution versus a multimodal distribution of `KneeScore_Patient`, which is the score assigned by `Patient`.
Our explanation of this is that `Surgeon` assigns any number between 1 and 100 without any discretion whereas `Patient` has a tendency to round up the score to the nearest fifth (Ex: 5, 10, 35, 50, etc) which creates the multi-picks of the distribution. 

Begin by splitting the data and fitting a full additive model as a baseline.

```{r, fig.height=5, fig.width=12, warning=FALSE}
sample_idx = sample(1:nrow(knee), 1500)
train_data = knee[sample_idx, ]
test_data = knee[-sample_idx, ]
surgeon_full_additive_model = lm(KneeScore_Surgeon ~ ., data = train_data)
evaluate(surgeon_full_additive_model)
showPlots(surgeon_full_additive_model)
```

Next we will try to determine which predictors are significant by comparing the full model to models each with a single predictor removed.  Evaluation of this code is set to false for convienience.  Comments show the significance of each test.

```{r, eval=FALSE}
surgeon_additive_model = lm(KneeScore_Surgeon ~ .-Surgeon, data = train_data)
Gender_additive_model = lm(KneeScore_Surgeon ~ .-Gender, data = train_data)
Race_additive_model = lm(KneeScore_Surgeon ~ .- Race, data = train_data)
Year_additive_model = lm(KneeScore_Surgeon ~ .- Year, data = train_data)
Age_additive_model = lm(KneeScore_Surgeon ~ .- Age, data = train_data)
Weight_additive_model = lm(KneeScore_Surgeon ~ .- Weight, data = train_data)
Height_additive_model = lm(KneeScore_Surgeon ~ .- Height, data = train_data)
BMI_additive_model = lm(KneeScore_Surgeon ~ .- BMI, data = train_data)
Diagnosis_additive_model = lm(KneeScore_Surgeon ~ .- Diagnosis, data = train_data)
Side_additive_model = lm(KneeScore_Surgeon ~ .- Side, data = train_data)
KneeScore_Patient_additive_model = lm(KneeScore_Surgeon ~ .- KneeScore_Patient, data = train_data)
GPS_additive_model = lm(KneeScore_Surgeon ~ .- GPS, data = train_data)
Manufacturer_additive_model = lm(KneeScore_Surgeon ~ .- Manufacturer, data = train_data)
FemoralComponentModel_additive_model = lm(KneeScore_Surgeon ~ .- FemoralComponentModel, data = train_data)
FemoralComponentSize_additive_model = lm(KneeScore_Surgeon ~ .- FemoralComponentSize, data = train_data)
FemoralComponentType_additive_model = lm(KneeScore_Surgeon ~ .- FemoralComponentType, data = train_data)
TibialTrayModel_additive_model = lm(KneeScore_Surgeon ~ .- TibialTrayModel, data = train_data)
TibialTraySize_additive_model = lm(KneeScore_Surgeon ~ .- TibialTraySize, data = train_data)
TibialInsertModel_additive_model = lm(KneeScore_Surgeon ~ .- TibialInsertModel, data = train_data)
TibialInsertWidth_additive_model = lm(KneeScore_Surgeon ~ .- TibialInsertWidth, data = train_data)
TibialInsertType_additive_model = lm(KneeScore_Surgeon ~ .- TibialInsertType, data = train_data)
PatellaModel_additive_model = lm(KneeScore_Surgeon ~ .- PatellaModel, data = train_data)
PatellaDiameter_additive_model = lm(KneeScore_Surgeon ~ .- PatellaDiameter, data = train_data)

anova(surgeon_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.2061
anova(Gender_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.3678
anova(Race_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.3949 
anova(Year_additive_model,surgeon_full_additive_model) ## * Some significance Pr(<F) 0.039
anova(Age_additive_model,surgeon_full_additive_model) ## *** Significant Pr(<F) 1.191e-11
anova(Weight_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.4295
anova(Height_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.6232
anova(BMI_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.4046
anova(Diagnosis_additive_model,surgeon_full_additive_model) ## Marginally Significant Pr(<F) 0.067
anova(Side_additive_model,surgeon_full_additive_model) ## *** Significant Pr(<F) 2.2e-16
anova(KneeScore_Patient_additive_model,surgeon_full_additive_model) ## *** Significant Pr(<F) 2.2e-16
anova(GPS_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.4046
anova(Manufacturer_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.7153
anova(FemoralComponentModel_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.1388
anova(FemoralComponentSize_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.3345
anova(FemoralComponentType_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.5606
anova(TibialTrayModel_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.8707
anova(TibialTraySize_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.1527
anova(TibialInsertModel_additive_model,surgeon_full_additive_model) ## * Some significance Pr(<F) 0.043
anova(TibialInsertWidth_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.721
anova(TibialInsertType_additive_model,surgeon_full_additive_model) ## Not significant Pr(<F) 0.613
```

Since there are only a few predictors selected as significant, we can create a smaller model based on those.

```{r}
surgeon_selective_model = lm(KneeScore_Surgeon ~ Age + Year + Diagnosis + Side + KneeScore_Patient + TibialInsertModel, data = train_data)
evaluate(surgeon_selective_model)
```

```{r}
anova(surgeon_selective_model, surgeon_full_additive_model)
```

The evaluation doesn't show much improvement and the large anova value suggests we shouldn't reject the null hypothesis.

We will attempt to utilize AIC and BIC approaches and evaluate the results.

```{r}
surgeon_model_back_aic = step(surgeon_full_additive_model, direction = "backward", trace = 0)
evaluate(surgeon_model_back_aic)
anova(surgeon_model_back_aic, surgeon_full_additive_model)
```

Again, we are unable to select the smaller AIC selected model as a better model. 

```{r, fig.height=5, fig.width=12, warning=FALSE}
clean_dataset_aic = removeInfluential(surgeon_model_back_aic, train_data)
surgeon_full_additive_model_clean = lm(KneeScore_Surgeon ~ ., data = clean_dataset_aic)
surgeon_model_aic_clean = step(surgeon_full_additive_model_clean, direction = "backward", trace = 0)
evaluate(surgeon_model_aic_clean)
showPlots(surgeon_model_aic_clean)
anova(surgeon_model_aic_clean, surgeon_full_additive_model_clean)
```

Once again, the anova result is not significant for the AIC model but we have improved on both the $Adjusted R^2$ and Cross Validation values.

Next we will utilize a Bayesian Information Criterion approach and evaluate the selected model.

```{r}
n = length(resid(surgeon_full_additive_model))
surgeon_model_back_bic = step(surgeon_full_additive_model, direction = "backward", k = log(n), trace = 0)
evaluate(surgeon_model_back_bic)
coef(surgeon_model_back_bic)
anova(surgeon_model_back_bic, surgeon_full_additive_model)
```

The BIC p-value is significant, so we select the smaller model.

Let's try the interaction between the predictors which we got in the model selected by Backward BIC.

```{r}
surgeon_model_back_bic_int = lm(KneeScore_Surgeon ~ Age * Side * KneeScore_Patient, data = train_data)
evaluate(surgeon_model_back_bic_int)
anova(surgeon_model_back_bic_int, surgeon_full_additive_model)
coef(surgeon_model_back_bic_int)
```

That approach didn't help us much.  Perhaps removing some influential data points will assist in creating a better model.  Next we remove influential points that satisfy the criteria for `Large Cook's Distance` values and evaluate BIC again.

```{r}
clean_dataset = removeInfluential(surgeon_model_back_bic, train_data)
surgeon_full_additive_model_clean = lm(KneeScore_Surgeon ~ ., data = clean_dataset)
surgeon_model_back_bic_clean = step(surgeon_full_additive_model_clean, direction = "backward", k = log(n), trace = 0)

evaluate(surgeon_model_back_bic_clean)
anova(surgeon_model_back_bic_clean, surgeon_full_additive_model_clean)
```

We get a definative p-value and can confidently select the smaller model.

```{r}
surgeon_model_back_bic_int_clean = lm(KneeScore_Surgeon ~ Age * Side * KneeScore_Patient, data = clean_dataset)
evaluate(surgeon_model_back_bic_int_clean)
```

Now we seem to be making some progress.  The interaction model on the dataset without large Cook's Distance values shaves a couple points off our cross validation score and increases $Adjusted R^2$.

Finally, we will compare the BIC and Interaction Models. 

```{r}
anova(surgeon_model_back_bic_clean, surgeon_model_back_bic_int_clean)
```

Based on Anova test we can reject the `surgeon_model_back_bic_clean` model and prefer `surgeon_model_back_bic_int_clean`.

Next, we will evaluate the RMSE of the best performing models thus far.

```{r echo=FALSE}
options(warn=-1)

full_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_full_additive_model, test_data))
full_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_full_additive_model, train_data))

sel_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_selective_model, test_data))
sel_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_selective_model, train_data))

clean_full_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_full_additive_model_clean, test_data))
clean_full_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_full_additive_model_clean, train_data))

aic_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_model_back_aic, test_data))
aic_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_model_back_aic, train_data))

clean_aic_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_model_aic_clean, test_data))
clean_aic_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_model_aic_clean, train_data))

bic_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_model_back_bic, test_data))
bic_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_model_back_bic, train_data))

int_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_model_back_bic_int, test_data))
int_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_model_back_bic_int, train_data))

clean_bic_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_model_back_bic_clean, test_data))
clean_bic_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_model_back_bic_clean, train_data))

clean_int_bic_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_model_back_bic_int_clean, test_data))
clean_int_bic_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_model_back_bic_int_clean, train_data))
```

| Model        | Train RMSE         | Test RMSE       |
|--------------|--------------------|-----------------|
| `Model All+` | `r full_train`     | `r full_test`   |
| `Selective Model`   | `r sel_train`      | `r sel_test`    |
| `Model AIC`  | `r aic_train`      | `r aic_test`    |
| `Model BIC`  | `r bic_train`      | `r bic_test`    |
| `Model Int`  | `r int_train `     | **`r int_test`**    |
| `Model Clean All+`  | `r clean_full_train ` | `r clean_full_test`|
| `Model Clean AIC`  | `r clean_aic_train ` | `r clean_aic_test`|
| `Model Clean BIC`  | `r clean_bic_train ` | `r clean_bic_test`|
| `Model Clean Int`  | `r clean_int_bic_train ` | `r clean_int_bic_test`|

The interaction model gives the BEST RMSE with low Cross Validation and High $Adjusted R^2$, so we will consider this as the best model to predict the KneeScore_Surgeon.

### Knee Score Patient

Begin by fitting a full additive model as a baseline and evaluating.

```{r, fig.height=5, fig.width=12}
pat_full_additive_model = lm(KneeScore_Patient ~ ., data = train_data)
evaluate(pat_full_additive_model)
```

Next we will try to determine which predictors are significant by comparing the full model to models each with a single predictor removed.  Evaluation of this code is set to false for convienience.  Comments show the significance of each test.

```{r, eval=FALSE}
patient_full_additive_model = lm(KneeScore_Patient ~ ., data = knee)
summary(patient_full_additive_model)
surgeon_additive_model = lm(KneeScore_Patient ~ .-Surgeon, data = knee)
Gender_additive_model = lm(KneeScore_Patient ~ .-Gender, data = knee)
Race_additive_model = lm(KneeScore_Patient ~ .- Race, data = knee)
Year_additive_model = lm(KneeScore_Patient ~ .- Year, data = knee)
Age_additive_model = lm(KneeScore_Patient ~ .- Age, data = knee)
Weight_additive_model = lm(KneeScore_Patient ~ .- Weight, data = knee)
Height_additive_model = lm(KneeScore_Patient ~ .- Height, data = knee)
BMI_additive_model = lm(KneeScore_Patient ~ .- BMI, data = knee)
Diagnosis_additive_model = lm(KneeScore_Patient ~ .- Diagnosis, data = knee)
Side_additive_model = lm(KneeScore_Patient ~ .- Side, data = knee)
KneeScore_Surgeon_additive_model = lm(KneeScore_Patient ~ .- KneeScore_Surgeon, data = knee)
GPS_additive_model = lm(KneeScore_Patient ~ .- GPS, data = knee)
Manufacturer_additive_model = lm(KneeScore_Patient ~ .- Manufacturer, data = knee)
FemoralComponentModel_additive_model = lm(KneeScore_Patient ~ .- FemoralComponentModel, data = knee)
FemoralComponentSize_additive_model = lm(KneeScore_Patient ~ .- FemoralComponentSize, data = knee)
FemoralComponentType_additive_model = lm(KneeScore_Patient ~ .- FemoralComponentType, data = knee)
TibialTrayModel_additive_model = lm(KneeScore_Patient ~ .- TibialTrayModel, data = knee)
TibialTraySize_additive_model = lm(KneeScore_Patient ~ .- TibialTraySize, data = knee)
TibialInsertModel_additive_model = lm(KneeScore_Patient ~ .- TibialInsertModel, data = knee)
TibialInsertWidth_additive_model = lm(KneeScore_Patient ~ .- TibialInsertWidth, data = knee)
TibialInsertType_additive_model = lm(KneeScore_Patient ~ .- TibialInsertType, data = knee)
PatellaModel_additive_model = lm(KneeScore_Patient ~ .- PatellaModel, data = knee)
PatellaDiameter_additive_model = lm(KneeScore_Patient ~ .- PatellaDiameter, data = knee)
anova(surgeon_additive_model,patient_full_additive_model)  ## Not significant Pr(>F) 0.3327
anova(Gender_additive_model,patient_full_additive_model)  ## Significant Pr(>F) 0.001174 **
anova(Race_additive_model,patient_full_additive_model)  ## Some significant Pr(>F) 0.03625 *
anova(Year_additive_model,patient_full_additive_model)   ## Not significant Pr(>F) 0.3122
anova(Age_additive_model,patient_full_additive_model)  ## Significant Pr(>F) 2.2e-16 ***
anova(Weight_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.06985
anova(Height_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.05945
anova(BMI_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.7246
anova(Diagnosis_additive_model,patient_full_additive_model)  ## Some significance Pr(>F) 0.03575
anova(Side_additive_model,patient_full_additive_model)  ## Not significant Pr(>F) 0.5652
anova(KneeScore_Surgeon_additive_model,patient_full_additive_model)  ## Significant Pr(>F) 2.2e-16 ***
anova(GPS_additive_model,patient_full_additive_model)  ## Not significant Pr(>F) 0.2498
anova(Manufacturer_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.3298
anova(FemoralComponentModel_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.1546
anova(FemoralComponentSize_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.1138
anova(FemoralComponentType_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.7325
anova(TibialTrayModel_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.1683
anova(TibialTraySize_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.6308
anova(TibialInsertModel_additive_model,patient_full_additive_model)  ## Not significant Pr(>F) 0.06147
anova(TibialTraySize_additive_model,patient_full_additive_model)  ## Not significant Pr(>F) 0.6308
anova(TibialInsertModel_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.06147
anova(TibialInsertWidth_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.3909
anova(TibialInsertType_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.4428
anova(PatellaModel_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.2525
anova(PatellaDiameter_additive_model,patient_full_additive_model) ## Not significant Pr(>F) 0.15
```

```{r, fig.height=5, fig.width=12}
pat_selective_model = lm(KneeScore_Patient ~ . - Diagnosis - Race - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = train_data)
evaluate(pat_selective_model)
```

```{r}
anova(pat_selective_model, pat_full_additive_model)
```

Backward AIC
```{r, fig.height=5, fig.width=12}
pat_aic = step(pat_full_additive_model, direction = "backward", trace = 0)
evaluate(pat_aic)
anova(pat_aic, pat_full_additive_model)
```

The anova results show to reject the AIC model when compared to the full model.

Backward AIC - Cleaned
```{r, fig.height=5, fig.width=12}
clean_data_aic = removeInfluential(pat_aic, train_data)
pat_full_additive_model_clean = lm(KneeScore_Patient ~ ., data = clean_data_aic)
pat_aic_clean = step(pat_full_additive_model_clean, direction = "backward", trace = 0)
evaluate(pat_aic_clean)
anova(pat_aic_clean, pat_full_additive_model_clean)
```

The AIC cleaned model yields an adjusted $R^2$ value of 0.2901856 and an cross validation of 15.42217 which is  better than pat_aic. However, when comparing the cleaned AIC model with the cleaned full additive model, the anova test rejects the cleaned AIC model.

Backward BIC

```{r, fig.height=5, fig.width=12}
n = length(resid(pat_full_additive_model))
pat_bic = step(pat_full_additive_model, direction = "backward", k = log(n), trace = 0)
evaluate(pat_bic)
anova(pat_bic, pat_full_additive_model)
```

The BIC model results in an adjusted $R^2$ value of 0.2458513 and and CV of 16.87071 When compared to the full model, Anova prefers the BIC model.

Backward BIC - Cleaned
```{r, fig.height=5, fig.width=12}
clean_data_bic = removeInfluential(pat_bic, train_data)
pat_full_additive_model_clean_bic = lm(KneeScore_Patient ~ ., data = clean_data_bic)
n = length(resid(pat_full_additive_model_clean_bic))
pat_bic_clean = step(pat_full_additive_model_clean_bic, direction = "backward", k = log(n), trace = 0)
evaluate(pat_bic_clean)
anova(pat_bic_clean, pat_full_additive_model_clean_bic)
```

The cleaned BIC results shows an improvement in the adjusted $R^2$ value and CV 0.2638423 and 13.58247 respectively. Anova results show to accept the cleaned BIC model as opposed to the cleaned full model.

Interaction Models
```{r, fig.height=5, fig.width=12}
pat_bic_int = lm(KneeScore_Patient ~ Age * Weight * Height * Gender * KneeScore_Surgeon, data = train_data)
evaluate(pat_bic_int)
anova(pat_bic_int, pat_full_additive_model )
```

Interaction Models - Cleaned
```{r, fig.height=5, fig.width=12}
pat_bic_int_clean = lm(KneeScore_Patient ~ Age * Weight * Height * Gender * KneeScore_Surgeon, data = clean_data_bic)
evaluate(pat_bic_int_clean)
anova(pat_bic_int_clean, pat_bic_clean)
```

The cleaned interaction model results in an adjusted $R^2$ value of 0.2612025 and CV of 13.6915. The anova test rejects the cleaned interaction model when compared to the BIC model.

Next, we will evaluate the RMSE of the best performing models thus far.

```{r eval=FALSE}
options(warn=-1)

full_test = rmse(test_data$KneeScore_Patient, predict(pat_full_additive_model, test_data))
full_train = rmse(train_data$KneeScore_Patient, predict(pat_full_additive_model, train_data))

sel_test = rmse(test_data$KneeScore_Patient, predict(pat_selective_model, test_data))
sel_train = rmse(train_data$KneeScore_Patient, predict(pat_selective_model, train_data))

clean_full_test = rmse(test_data$KneeScore_Patient, predict(pat_full_additive_model_clean, test_data))
clean_full_train = rmse(train_data$KneeScore_Patient, predict(pat_full_additive_model_clean, train_data))

aic_test = rmse(test_data$KneeScore_Patient, predict(pat_aic, test_data))
aic_train = rmse(train_data$KneeScore_Patient, predict(pat_aic, train_data))

clean_aic_test = rmse(test_data$KneeScore_Patient, predict(pat_aic_clean, test_data))
clean_aic_train = rmse(train_data$KneeScore_Patient, predict(pat_aic_clean, train_data))

bic_test = rmse(test_data$KneeScore_Patient, predict(pat_bic, test_data))
bic_train = rmse(train_data$KneeScore_Patient, predict(pat_bic, train_data))

int_test = rmse(test_data$KneeScore_Patient, predict(pat_bic_int, test_data))
int_train = rmse(train_data$KneeScore_Patient, predict(pat_bic_int, train_data))

clean_bic_test = rmse(test_data$KneeScore_Patient, predict(pat_bic_clean, test_data))
clean_bic_train = rmse(train_data$KneeScore_Patient, predict(pat_bic_clean, train_data))

clean_int_bic_test = rmse(test_data$KneeScore_Patient, predict(pat_bic_int_clean, test_data))
clean_int_bic_train = rmse(train_data$KneeScore_Patient, predict(pat_bic_int_clean, train_data))
```

| Model        | Train RMSE         | Test RMSE       |
|--------------|--------------------|-----------------|
| `Model All+` | `r full_train`     | `r full_test`   |
| `Selective Model`   | `r sel_train`      | `r sel_test`    |
| `Model AIC`  | `r aic_train`      | `r aic_test`    |
| `Model BIC`  | `r bic_train`      | `r bic_test`    |
| `Model Int`  | `r int_train `     | `r int_test`    |
| `Model Clean All+`  | `r clean_full_train ` | `r clean_full_test`|
| `Model Clean AIC`  | `r clean_aic_train ` | `r clean_aic_test`|
| `Model Clean BIC`  | `r clean_bic_train ` | `r clean_bic_test`|
| `Model Clean Int`  | `r clean_int_bic_train ` | `r clean_int_bic_test`|


The interaction model is the best to predict KneeScore_Patient scores as it results in the lowest RMSE value.

## Conclusion

Unfortunately, we are not able to definitavely answer any of the questions we set out to answer.  While we did make incremental improvements with models on both response variables, the $Adjusted R^2$ values never reached levels where we could be confident about predictions.

## Appendix

```{r, eval = FALSE}
# Warning: takes hours to fit
all_two_way_model = lm(KneeScore_Surgeon ~ .*., data = knee)
```

```{r, eval = FALSE}
# Warning...
n = length(resid(all_two_way_model))
model_one_bic_all_two_way = step(all_two_way_model, direction = 'backward', k = log(n), trace = 0)
```

```{r, eval = FALSE}
model_one_pat = lm(log(KneeScore_Patient) ~ ., data = knee)
model_one_surg = lm(log(KneeScore_Surgoen) ~ ., data = knee)
```

```{r, eval = FALSE}
model_two_exp_pat= lm(KneeScore_Patient^2 ~ ., data = knee)
model_two_exp_surg = lm(KneeScore_Surgeon^2 ~ ., data = knee)
```

```{r, eval = FALSE}
# replace all 0 values with the mean of the vector
temp = knee
temp$KneeScore_Patient[temp$KneeScore_Patient == 0] = mean(temp$KneeScore_Patient)

model_three = lm(KneeScore_Patient ~ ., data = temp)
evaluate(model_five)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
clean_temp_data = removeLargeHatValues(model_three, temp)
model_four = lm(KneeScore_Patient ~ ., data = clean_temp_data)
evaluate(model_four)
```

```{r, eval = FALSE}
pat_model = lm(KneeScore_Patient ~. , data = knee)
pat_model_back_aic = step(pat_model, direction = "backward", trace = 0)
pat_model_for_aic = step(pat_model, direction = "forward", trace = 0)
pat_model_both_aic = step(pat_model, direction = "both", trace = 0)

n = length(resid(pat_model))
pat_model_back_bic = step(pat_model, direction = "backward", k = log(n), trace = 0)
pat_model_for_bic = step(pat_model, direction = "forward", k = log(n), trace = 0)
pat_model_both_bic = step(pat_model, direction = "both", k = log(n), trace = 0)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_back_aic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_for_aic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_both_aic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
## Adj.R.Squared: 0.24 - CrossValidation: 16.9
evaluate(pat_model_back_bic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_for_bic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_both_bic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
## Remove large leverages
new_data = removeInfluential(pat_model, knee)
pat_model_without_large_leverages = lm(KneeScore_Patient ~ ., data = new_data)
evaluate(pat_model_without_large_leverages)
```

Removing the records with large hat values and refitting simply introduces more large hat values.  What is unique about these rows of data?

```{r, eval = FALSE}
which(hatvalues(pat_model) == 1.0)
print(knee[17,])
print(knee[240,])
print(knee[638,])
print(knee[1093,])
print(knee[1775, ])
```


```{r, fig.height=5, fig.width=12, eval = FALSE}
pat_model_nine = lm(KneeScore_Patient ~ . - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - FemoralComponentType, data = knee)
evaluate(pat_model_nine)
```

## References
---
references:
- id: fenner2012a
  title: One-click science marketing
  author:
  - family: Fenner
    given: Martin
  container-title: Nature Materials
  volume: 11
  URL: 'http://dx.doi.org/10.1038/nmat3283'
  DOI: 10.1038/nmat3283
  issue: 4
  publisher: Nature Publishing Group
  page: 261-263
  type: article-journal
  issued:
    year: 2012
    month: 3
---
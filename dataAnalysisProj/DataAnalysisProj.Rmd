---
title:  'Data Analysis Project: Knee Replacement Surgery Outcomes'
subtitle: 'Analysis Project'
date: "December 14, 2016"
output:
  html_document:
    css: custom.css
    toc: yes
author:
- Gitika Jain (gitikaj2)
- Heather Sewak (hsewak2)
- Ionel Miu (imiu2)
- Jared Michael Colbert (jaredc2)
---
#  Knee Replacement Surgery Outcome

## Introduction
This research aims to determine if the postoperative function of a total knee replacement recipient can be modeled based on anthropomorphic data, and details of the surgical procedure. We decided to carry out testing on models with two postoperative scoring systems as the response variable.  The first score is the "Surgeon's Score" which measures more objective postoperative metrics about the condition of the replaced knee following surgery.  These metrics include: how straight or how flexed the patient can make their knee, how well aligned the knee appears when inspecting from a frontal perspective, and how much laxity or separation can be achieved by placing the lower leg under tension.  Finally, the patient is asked about the overall pain they experience with the knee.  The resulting score is a value between 0-100 with 80-100 being an excellent result.

The second score we modeled is the "Patient's Score" which is generated by three questions: how far are they able to walk, how well can they navigate stairs, and do they require any walking aids (crutches, cane, etc).

We hope to create a model that can predict each of the scores based on a variety of categorical and numeric predictors. 

## Background information
This data has been collected from a Southern California Hospital in collaboration with the Shiley Center for Orthopedic Research.  The data is used for both clinical research and postoperative monitoring of implant survivorship.  All patient specific information has been removed from the data so it adheres to the Health Insurance Portability and Accountability Act (HIPAA).

## Overview of the work flow
We formed our group by advertising on the course forum site and Slack chat.
After the group had been formed, the first step was to decide on a topic and a data set for the selected topic.
Next, after our proposal had been approved, we cleaned up the data and started working towards finding a final model that is most suitable for our data set.
To reach the final model, each of us, worked independently to come up with a list of models.
Models were presented and discussed during our regular meetings via Zoom where ideas were exchanged.
Eventaully, we converged and decided on a final model that has been used for the work presented here.

## Statement
We hope to determine the impact a particular surgeon or the type of implants used has on the patient's outcome after surgery.  We are also interested in exploring the potential relationship between anthropomorphic factors and the patient's postoperative satisfaction.  Finally, we would like to understand if the considerable expense of computer navigation in knee replacement leads to better postoperative scores. Nevertheless, we keep our options opened to investigate other aspects that were not 'visible' or thought about but might reveal themselves as we move on with the project.

Here we may consider *KneeScore\_Surgeon* or *KneeScore\_Patient* as response and *Age, Gender, Weight, Height, BMI, Race, Side, Manufacturer* as predictors. *Gender, Race, Side and Manufacturer* are some of the categorical predictors and *Age, Height, Weight and BMI* are numerical predictors.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lmtest)
library(car)
purple = '#772299'; green = '#3FE500'; red = '#E50000'; lightblue = '#229999'; blue = '#336699'
```

```{r}
knee = read.csv('knees.csv')
knee$Surgeon = as.factor(knee$Surgeon)
knee$Year = as.factor(knee$Year)
```

### Functions for model evaluation

```{r}
#Function to plot various plots as QQPlot, Fitted vs Residual Plot.
plots = function(model) {
  par(mfcol=c(1,3))
  qqnorm(resid(model), col = red, pch = 16)
  qqline(resid(model), col = lightblue, lwd = 2)
  plot(fitted(model), resid(model), xlab = 'Fitted Values', ylab = 'Residuals', main = 'Fitted vs. Residuals Plot', col = purple, pch = 16)
  abline(h = 0, col = green, lwd = 2)
  hist(resid(model), col = blue, main = 'Histogram of Residuals', xlab = 'Residuals')
}

#Function to calculate various statistics which we use to evaluate the model
evaluate = function(model){
  # Run each of the tests
  lambda = 1.000001 # this is a hack so the function will return a CrossValidation score - HatValues of 1.0 are causing a division by zero error.
  shapiroTest = shapiro.test(resid(model))
  bpTest = bptest(model)
  rSquared = summary(model)$r.squared
  adjRSquared = summary(model)$adj.r.squared
  loocv = sqrt(mean((resid(model) / (lambda - hatvalues(model))) ^ 2))
  large_hat = sum(hatvalues(model) > 2 * mean(hatvalues(model)))
  large_resid = sum(rstandard(model)[abs(rstandard(model)) > 2]) / (length(rstandard(model) + lambda))
  
  
  # Collect tests in dataframe
  values = data.frame(Result = c(prettyNum(shapiroTest$p.value), prettyNum(bpTest$p.value), prettyNum(rSquared), prettyNum(adjRSquared),prettyNum(loocv),               prettyNum(large_hat))) 
  rowNames = data.frame(Test = c('Shapiro Wilk pvalue', 'Breusch-Pagan pvalue', 'R Squared', 'Adj R Squared', 'Cross Validation', 'Large Hat Values'))

  summary_output = cbind(rowNames, values)
  show(summary_output)
  plots(model)
}
# find and remove all influential values and return the resulting dataframe
removeInfluential = function(model, data){
  large_lev_index = which(hatvalues(model) > 2 * mean(hatvalues(model)))
  cooks = which(cooks.distance(model) > 4 / length(cooks.distance(model)))
  rStandard = abs(rstandard(model))
  rStandardIndex = which(rStandard > 2)
  index_ = union(union(large_lev_index, cooks), rStandardIndex)
  newData = data[-index_ , ]
  return(newData)
}

# identify all influential data points
IdentifyInfluential = function(model, data){
  large_lev_index = which(hatvalues(model) > 2 * mean(hatvalues(model)))
  cooks = which(cooks.distance(model) > 4 / length(cooks.distance(model)))
  rStandard = abs(rstandard(model))
  rStandardIndex = which(rStandard > 2)
  index_ = union(union(large_lev_index, cooks), rStandardIndex)
  return(index_)
}

# find large hat values, subtract them from the dataframe and return the dataframe
removeLargeHatValues = function(model, data){
  largeHat = which(hatvalues(model) > 2 * mean(hatvalues(model)))
  newData = data[-largeHat, ]
  return(newData)
}
# calculate the prediction accuracy with a margin (i.e. if the margin is set to 2, then a predicted value of 35.5 will return true if the actual value is 33.5-37.5)
prediction_accuracy = function(model, data, margin) {
  upper = data + margin
  lower = data - margin
  prediction = predict(model, newdata = data)
  return(sum(prediction < upper & lower < prediction)) / length(prediction)
}

rmse  = function(actual, predicted) {  sqrt(mean((actual - predicted) ^ 2))}

```

### Knee Score Surgeon

Before moving onto the formal analysis of our data set, we started by performing visual inspection by plotting dependencies among our the variables.
The main focus was to look for any visual relationship between `KneeScore_Surgeon, KneeScore_Patient` and the rest of the variables. 
Our choice was to use `scatterplotMatrix` from the `car` package as it plots regression lines along with the actual data.

```{r, fig.height=10, fig.width=12, warning=FALSE}
scatterplotMatrix(~ KneeScore_Surgeon + KneeScore_Patient + Surgeon + Year + Age + Gender + Weight + Height, span=0.7, data=knee)
```

```{r, fig.height=10, fig.width=12, warning=FALSE}
scatterplotMatrix(~ KneeScore_Surgeon + KneeScore_Patient + BMI + Diagnosis + Race + Side + GPS + Manufacturer, span=0.7, data=knee)
```

```{r, fig.height=10, fig.width=12, warning=FALSE}
scatterplotMatrix(~ KneeScore_Surgeon + KneeScore_Patient + FemoralComponentModel + FemoralComponentSize + FemoralComponentType + TibialTrayModel + TibialInsertType, span=0.7, data=knee)
```

```{r, fig.height=10, fig.width=12, warning=FALSE}
scatterplotMatrix(~ KneeScore_Surgeon + KneeScore_Patient + TibialTraySize + TibialInsertModel + TibialInsertWidth + PatellaModel + PatellaDiameter, span=0.7, data=knee)
```

**Comments:** Based on the above graphics, `KneeScore_Surgeon, KneeScore_Patient` seems to behave similarly across all variables. This is expected as both, surgeon and patient, should be in consensus in regards to the success or quality of a procedure.

For a better view of the above statement, we plotted the following plot, showing a strong relationship between `KneeScore_Surgeon` and `KneeScore_Patient`.

```{r, fig.height=10, fig.width=12, warning=FALSE}
scatterplotMatrix(~ KneeScore_Surgeon + KneeScore_Patient, span=0.7, data=knee)
```

The distribution of `KneeScore_Surgeon` which reflects the score of a procedure assigned be the `Surgeon` is unimodal distribution versus a multimodal distribution of `KneeScore_Patient`, which is the score assigned by `Patient`.
Our explanation of this is that `Surgeon` assigns any number between 1 and 100 without any discretion whereas `Patient` has a tendency to round up the score to the nearest fifth (Ex: 5, 10, 35, 50, etc) which creates the multi-picks of the distribution. 

Begin by fitting a full additive model as a baseline and evaluating.

```{r, fig.height=5, fig.width=12}
surgeon_full_additive_model = lm(KneeScore_Surgeon ~ ., data = knee)
evaluate(surgeon_full_additive_model)
```

The baseline model has many large hat values.  Let's try to understand why.

```{r}
largeHat = which(hatvalues(surgeon_full_additive_model) > (2 * mean(hatvalues(surgeon_full_additive_model))))
unique(largeHat)
```

```{r}
print(knee[689, ])
print(knee[1927, ])
```

After inspection of the rows that represent the large hat values it seems that there are a preponderance of values that are either errant entries or valid, but rare entries.  For example, the `Race` predictor has only two entries for `Mid-East Arabian` and the `FemoralComponentSize` entry of '8+' is errant.  First we will attempt to remove the predictors that seem to be prone to errant or rare values and later we will remove the high leverage values directly.

We attempted several model iterations with transformations of both the response and predictors, as well as large all two-way interaction models none of which proved helpful in predicting the response.  The models with many interactions took both substantial time and computing resources to fit and were suspect of overfitting.  We pick up here with model eight which was the first of the promising models.  The interested reader can find earlier attempted models in the appendix.

```{r, fig.height=5, fig.width=12}
surgeon_model_eight = lm(KneeScore_Surgeon ~ . - Diagnosis - Race - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = knee)
evaluate(surgeon_model_eight)
```

```{r}
anova(surgeon_model_eight, surgeon_full_additive_model)
```

While removing the predictors doesn't yield a definitively better model at a reasonable $\alpha$, it does increase adjusted $R^2$, and slightly improves the cross validation score.

Utilize a Bayesian Information Criterion approach and evaluate the selected model.

```{r, fig.height=5, fig.width=12}
n = length(resid(surgeon_full_additive_model))
surgeon_model_back_bic = step(surgeon_full_additive_model, direction = "backward", k = log(n), trace = 0)
evaluate(surgeon_model_back_bic)
anova(surgeon_model_back_bic, surgeon_full_additive_model)
```

The resulting BIC model scores marginally better in cross validation though the Anova results clearly prefer the full additive model.

Next, we will evaluate the RMSE of the best performing models thus far.

```{r}
options(warn=-1)
sample_idx = sample(1:nrow(knee), 1500)
train_data = knee[sample_idx, ]
test_data = knee[-sample_idx, ]


full_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_full_additive_model, test_data))
full_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_full_additive_model, train_data))

test_mod_8 = rmse(test_data$KneeScore_Surgeon, predict(surgeon_model_eight, test_data))
train_mod_8 = rmse(train_data$KneeScore_Surgeon, predict(surgeon_model_eight, train_data))

bic_test = rmse(test_data$KneeScore_Surgeon, predict(surgeon_model_back_bic, test_data))
bic_train = rmse(train_data$KneeScore_Surgeon, predict(surgeon_model_back_bic, train_data))
```

| Model        | Train RMSE         | Test RMSE       |
|--------------|--------------------|-----------------|
| `Model All+` | `r full_train`     | `r full_test`   |
| `Model 8`    | `r train_mod_8`    | `r test_mod_8`  |
| `Model BIC`  | `r bic_train`      | `r bic_test`    |

The full model delivers the best RMSE as well. 

Perhaps removing some influential data points will assist in creating a better model.  Next we remove influential points that satisfy the criteria for `Large Leverage`, `Large Cook's Distance` and `Large RStandard` values.

```{r, fig.height=5, fig.width=12}
new_dataframe = removeInfluential(surgeon_model_eight, knee)
surgeon_model_nine = lm(KneeScore_Surgeon ~ . - Diagnosis - Race - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = new_dataframe)
evaluate(surgeon_model_nine)
```

This does seem to be of moderate benefit in relation to adjusted $R^2$ and cross validation.

We will evaluate the same model once again after removing the identified large hat values.

```{r, fig.height=5, fig.width=12}
remove_hat_dataframe = removeLargeHatValues(surgeon_model_nine, new_dataframe)
surgeon_model_ten = lm(KneeScore_Surgeon ~ . - Diagnosis - Race - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = remove_hat_dataframe)
evaluate(surgeon_model_ten)
```

Removing the large hat values decreased the cross validation score.  Perhaps we can find a better model by running BIC on the modified dataframe.

```{r, fig.height=5, fig.width=12}
n = length(resid(surgeon_model_ten))
surgeon_model_10_bic = step(surgeon_model_ten, direction = "backward", k = log(n), trace = 0)
evaluate(surgeon_model_10_bic)
```

Despite the reduction in leveraged values the selected model is no better than preceding models.

```{r}
sample_idx_two = sample(1:nrow(remove_hat_dataframe), 1300)
train_data_two = remove_hat_dataframe[sample_idx_two, ]
test_data_two = remove_hat_dataframe[-sample_idx_two, ]

test_mod_10 = rmse(test_data_two$KneeScore_Surgeon, predict(surgeon_model_ten, train_data_two))
train_mod_10 = rmse(train_data_two$KneeScore_Surgeon, predict(surgeon_model_ten, train_data_two))

test_10_bic = rmse(test_data_two$KneeScore_Surgeon, predict(surgeon_model_10_bic, train_data_two))
train_10_bic = rmse(train_data_two$KneeScore_Surgeon, predict(surgeon_model_10_bic, train_data_two))
```

| Model         | Train RMSE         | Test RMSE       |
|---------------|--------------------|-----------------|
| `Model 10`    | `r train_mod_10`   | `r test_mod_10` |
| `Model 10 BIC`| `r train_10_bic`   | `r test_10_bic` |

Removing the large hat values has had the opposite intended effect of decreasing the RMSE values.

### Knee Score Patient

Begin by fitting a full additive model as a baseline and evaluating.

```{r, fig.height=5, fig.width=12}
pat_full_additive_model = lm(KneeScore_Patient ~ ., data = knee)
evaluate(pat_full_additive_model)
```

The baseline model has many large leverage values, which, upon inspection may be coming from several of the implant fields.  Perhaps removing the suspect implant predictors will help improve the model.

```{r, fig.height=5, fig.width=12}
pat_model_eight = lm(KneeScore_Patient ~ . - Diagnosis - Race - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = knee)
evaluate(pat_model_eight)
```

```{r}
anova(pat_model_eight, pat_full_additive_model)
```

While removing the predictors doesn't yield a definitively better model at a reasonable $\alpha$, it does increase adjusted $R^2$ and slightly improves the cross validation score.

Utilize a Bayesian Information Criterion approach and evaluate the selected model.

```{r, fig.height=5, fig.width=12}
n = length(resid(pat_full_additive_model))
pat_model_back_bic = step(pat_full_additive_model, direction = "backward", k = log(n), trace = 0)
evaluate(pat_model_back_bic)
anova(pat_model_back_bic, pat_full_additive_model)
```

The resulting BIC model scores marginally better in cross validation though the Anova results clearly prefer the full additive model.

Next, we will evaluate the RMSE of the best performing models thus far.

```{r}
options(warn=-1)
sample_idx = sample(1:nrow(knee), 1500)
train_data = knee[sample_idx, ]
test_data = knee[-sample_idx, ]

rmse  = function(actual, predicted) {  sqrt(mean((actual - predicted) ^ 2))}

pat_full_test = rmse(test_data$KneeScore_Patient, predict(pat_full_additive_model, test_data))
pat_full_train = rmse(train_data$KneeScore_Patient, predict(pat_full_additive_model, train_data))

pat_test_mod_8 = rmse(test_data$KneeScore_Patient, predict(pat_model_eight, test_data))
pat_train_mod_8 = rmse(train_data$KneeScore_Patient, predict(pat_model_eight, train_data))

pat_bic_test = rmse(test_data$KneeScore_Patient, predict(pat_model_back_bic, test_data))
pat_bic_train = rmse(train_data$KneeScore_Patient, predict(pat_model_back_bic, train_data))
```

| Model        | Train RMSE         | Test RMSE       |
|--------------|--------------------|-----------------|
| `Model All+` | `r full_train`     | `r full_test`   |
| `Model 8`    | `r train_mod_8`    | `r test_mod_8`  |
| `Model BIC`  | `r bic_train`      | `r bic_test`    |

The additive model with all predictors has the best RMSE values.

Perhaps removing some influential data points will assist in creating a better model.  Next we remove influential points that satisfy the criteria for `Large Leverage`, `Large Cook's Distance` and `Large RStandard` values.

```{r, fig.height=5, fig.width=12}
new_dataframe = removeInfluential(pat_model_eight, knee)
pat_model_nine = lm(KneeScore_Patient ~ . - Diagnosis - Race - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = new_dataframe)
evaluate(pat_model_nine)
```

This does seem to be of moderate benefit in relation to adjusted $R^2$ and cross validation.

We will evaluate the same model once again after removing the identified large hat values.

```{r, fig.height=5, fig.width=12}
remove_hat_dataframe = removeLargeHatValues(pat_model_nine, new_dataframe)
pat_model_ten = lm(KneeScore_Patient ~ . - Diagnosis - Race - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = remove_hat_dataframe)
evaluate(pat_model_ten)
```

Removing the large hat values decreased the cross validation score.  Perhaps we can find a better model by running BIC on the modified dataframe.

```{r, fig.height=5, fig.width=12}
n = length(resid(pat_model_ten))
pat_model_10_bic = step(pat_model_ten, direction = "backward", k = log(n), trace = 0)
evaluate(pat_model_10_bic)
```

Despite the reduction in leveraged values the selected model is no better than preceding models.

```{r}
sample_idx_two = sample(1:nrow(remove_hat_dataframe), 1300)
train_data_two = remove_hat_dataframe[sample_idx_two, ]
test_data_two = remove_hat_dataframe[-sample_idx_two, ]

test_mod_10 = rmse(test_data_two$KneeScore_Patient, predict(pat_model_ten, train_data_two))
train_mod_10 = rmse(train_data_two$KneeScore_Patient, predict(pat_model_ten, train_data_two))

test_10_bic = rmse(test_data_two$KneeScore_Patient, predict(pat_model_10_bic, train_data_two))
train_10_bic = rmse(train_data_two$KneeScore_Patient, predict(pat_model_10_bic, train_data_two))
```

| Model         | Train RMSE         | Test RMSE       |
|---------------|--------------------|-----------------|
| `Model 10`    | `r train_mod_10`   | `r test_mod_10` |
| `Model 10 BIC`| `r train_10_bic`   | `r test_10_bic` |

Interestingly, again the models fit on the dataset without leverages fares worse on test RMSE.

## Conclusion



## Appendix

```{r, eval = FALSE}
# Warning: takes hours to fit
all_two_way_model = lm(KneeScore_Surgeon ~ .*., data = knee)
```

```{r, eval = FALSE}
# Warning...
n = length(resid(all_two_way_model))
model_one_bic_all_two_way = step(all_two_way_model, direction = 'backward', k = log(n), trace = 0)
```

```{r, eval = FALSE}
model_one_pat = lm(log(KneeScore_Patient) ~ ., data = knee)
model_one_surg = lm(log(KneeScore_Surgoen) ~ ., data = knee)
```

```{r, eval = FALSE}
model_two_exp_pat= lm(KneeScore_Patient^2 ~ ., data = knee)
model_two_exp_surg = lm(KneeScore_Surgeon^2 ~ ., data = knee)
```

```{r, eval = FALSE}
# replace all 0 values with the mean of the vector
temp = knee
temp$KneeScore_Patient[temp$KneeScore_Patient == 0] = mean(temp$KneeScore_Patient)

model_three = lm(KneeScore_Patient ~ ., data = temp)
evaluate(model_five)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
clean_temp_data = removeLargeHatValues(model_three, temp)
model_four = lm(KneeScore_Patient ~ ., data = clean_temp_data)
evaluate(model_four)
```

```{r, eval = FALSE}
pat_model = lm(KneeScore_Patient ~. , data = knee)
pat_model_back_aic = step(pat_model, direction = "backward", trace = 0)
pat_model_for_aic = step(pat_model, direction = "forward", trace = 0)
pat_model_both_aic = step(pat_model, direction = "both", trace = 0)

n = length(resid(pat_model))
pat_model_back_bic = step(pat_model, direction = "backward", k = log(n), trace = 0)
pat_model_for_bic = step(pat_model, direction = "forward", k = log(n), trace = 0)
pat_model_both_bic = step(pat_model, direction = "both", k = log(n), trace = 0)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_back_aic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_for_aic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_both_aic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
## Adj.R.Squared: 0.24 - CrossValidation: 16.9
evaluate(pat_model_back_bic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_for_bic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
evaluate(pat_model_both_bic)
```

```{r, fig.height=5, fig.width=12, eval = FALSE}
## Remove large leverages
new_data = removeInfluential(pat_model, knee)
pat_model_without_large_leverages = lm(KneeScore_Patient ~ ., data = new_data)
evaluate(pat_model_without_large_leverages)
```

Removing the records with large hat values and refitting simply introduces more large hat values.  What is unique about these rows of data?

```{r, eval = FALSE}
which(hatvalues(pat_model) == 1.0)
print(knee[17,])
print(knee[240,])
print(knee[638,])
print(knee[1093,])
print(knee[1775, ])
```


```{r, fig.height=5, fig.width=12, eval = FALSE}
pat_model_nine = lm(KneeScore_Patient ~ . - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - FemoralComponentType, data = knee)
evaluate(pat_model_nine)
```

## References

---
references:
- id: fenner2012a
  title: One-click science marketing
  author:
  - family: Fenner
    given: Martin
  container-title: Nature Materials
  volume: 11
  URL: 'http://dx.doi.org/10.1038/nmat3283'
  DOI: 10.1038/nmat3283
  issue: 4
  publisher: Nature Publishing Group
  page: 261-263
  type: article-journal
  issued:
    year: 2012
    month: 3
---


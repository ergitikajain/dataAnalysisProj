---
title:  'Data Analysis Project: Knee Replacement Surgery Outcomes'
subtitle: 'Analysis Project'
date: "December 14, 2016"
output:
  html_document:
    toc: yes
author:
- Gitika Jain (gitikaj2)
- Heather Sewak (hsewak2)
- Ionel Miu (imiu2)
- Jared Michael Colbert (jaredc2)
---
#  Knee Replacement Surgery Outcome

## Introduction
This research aims to determine if the postoperative function of a total knee replacement recipient can be modeled based on anthropomorphic data and details of the surgical procedure.  We decided to carry out testing on models with two postoperative scoring systems as the response variable.  The first score is the "Surgeon's Score" which measures more objective postoperative metrics about the condition of the replaced knee following surgery.  These metrics include: how straight or how flexed the patient can make thier knee, how well aligned the knee appears when inspecting from a frontal perspective and how much laxity or separation can be achieved by placing the lower leg under tension.  Finally, the patient is asked about the overall pain they experience with the knee.  The resulting score is a value between 0-100 with 80-100 being an excellent result.

The second score we model is the "Patient's Score" which is generated by three questions: how far are they able to walk, how well can they navigate stairs and do they require any walking aids (crutches, cane, etc).

We hope to create a model that can predict each of the scores based on a variety of categorical and numeric predictors. 

## Background information
This data has been collected from a Southern California Hospital in collaboration with the Shiley Center for Orthopedic Research.  The data is used for both clinical research and postoperative monitoring of implant survivorship.  All patient specific information has been removed from the data so it adheres to the Health Insurance Portability and Accountability Act (HIPAA).

## Statement
We hope to determine the impact a particular surgeon or the type of implants used has on the patient's outcome after surgery.  We are also interested in exploring the potential relationship between anthropomorphic factors and the patient's postoperative satisfaction.  Finally, we would like to understand if the considerable expense of computer navigation in knee replacement leads to better postoperative scores. Nevertheless, we keep our option opened to investigate other aspects that were not 'visibile' or never thought about it but might reveal themselfs as we move on with the project.

Here we may consider *KneeScore\_Surgeon* or *KennScore\_Patient* as response and *Age, Gendar, Weight, Height, BMI, Race, Side, Manufacturer* as predictors. *Gendar, Race, Side and Manufacturer* are some of the categorical predictors and *Age, Height, Weight and BMI* are numerical predictors.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(lmtest)
purple = '#772299'; green = '#3FE500'; red = '#E50000'; lightblue = '#229999'; blue = '#336699'
```

```{r}
knee = read.csv('knees.csv')
knee$Surgeon = as.factor(knee$Surgeon)
knee$Year = as.factor(knee$Year)
```

### Functions for model evaluation

```{r}
plots = function(model) {
  par(mfcol=c(2,2))
  qqnorm(resid(model), col = red, pch = 16)
  qqline(resid(model), col = lightblue, lwd = 2)
  plot(fitted(model), resid(model), xlab = 'Fitted Values', ylab = 'Residuals', main = 'Fitted vs. Residuals Plot', col = purple, pch = 16)
  abline(h = 0, col = green, lwd = 2)
  hist(resid(model), col = blue, main = 'Histogram of Residuals', xlab = 'Residuals')
}

evaluate = function(model){
  # Run each of the tests
  lambda = 1.000001 # this is a hack so the function will return a CrossValidation score - HatValues of 1.0 are causing a division by zero error.
  shapiroTest = shapiro.test(resid(model))
  bpTest = bptest(model)
  rSquared = summary(model)$r.squared
  adjRSquared = summary(model)$adj.r.squared
  loocv = sqrt(mean((resid(model) / (lambda - hatvalues(model))) ^ 2))
  large_lev = sum(hatvalues(model) > 2 * mean(hatvalues(model)))
  large_resid = sum(rstandard(model)[abs(rstandard(model)) > 2]) / (length(rstandard(model) + lambda))
  
  
  # Collect tests in dataframe
  values = data.frame(Result = c(prettyNum(shapiroTest$p.value), prettyNum(bpTest$p.value), prettyNum(rSquared), prettyNum(adjRSquared),prettyNum(loocv),               prettyNum(large_lev))) 
  rowNames = data.frame(Test = c('Shapiro Wilk pvalue', 'Breusch-Pagan pvalue', 'R Squared', 'Adj R Squared', 'Cross Validation', 'Large Leverages'))

  summary_output = cbind(rowNames, values)
  show(summary_output)
  plots(model)
}

removeInfluential = function(model, data){
  large_lev_index = which(hatvalues(model) > 2 * mean(hatvalues(model)))
  cooks = which(cooks.distance(model) > 4 / length(cooks.distance(model)))
  rStandard = abs(rstandard(model))
  rStandardIndex = which(rStandard > 2)
  index_ = intersect(intersect(large_lev_index, cooks), rStandardIndex)
  newData = data[-c(index_),]
  return(newData)
}

removeLargeHatValues = function(model, data){
  largeHat = which(hatvalues(model) > 2 * mean(hatvalues(model)))
  return(largeHat)
}

prediction_accuracy = function(model, data, margin) {
  upper = data + margin
  lower = data - margin
  prediction = predict(model, newdata = data)
  return(sum(prediction < upper & lower < prediction)) / length(prediction)
}
```

```{r}
model_eight = lm(KneeScore_Patient ~ . - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = knee)
new_dataframe = removeInfluential(model_eight, knee)
model_nine = lm(KneeScore_Patient ~ ., data = new_dataframe)
n = length(resid(model))
model_back_bic = step(model, direction = "backward", k = log(n), trace = 0)
```



###Knee Score Surgeon

```{r}
knee_model = lm(KneeScore_Surgeon ~ ., data = knee)
summary(knee_model)

knee_model_aic = step(knee_model, direction = "backward", trace = 0)
summary(knee_model_aic)$call

knee_model_bic = step(knee_model, direction = "backward", trace = 0)
summary(knee_model_bic)$call
```

```{r}
anova(knee_model_aic, knee_model)
```

```{r warning=FALSE, message=FALSE}
plot(fitted(knee_model_aic), resid(knee_model_aic), col = "magenta", pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h=0, lwd = 2, col = "olivedrab1")

qqnorm(resid(knee_model_aic), main = "Normal QQ Plot - Knee Model AIC", col = "purple4", pch = 20, cex = 1.5)
qqline(resid(knee_model_aic), col = "Orange", lwd = 3)

library(lmtest)
bptest(knee_model_aic)

shapiro.test(resid(knee_model_aic))

```

### Knee Score Patient   


```{r}
model_eight = lm(KneeScore_Patient ~ . - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = knee)
new_dataframe = removeInfluential(model_eight, knee)
model_nine = lm(KneeScore_Patient ~ ., data = new_dataframe)
n = length(resid(model))
model_back_bic = step(model, direction = "backward", k = log(n), trace = 0)
```

Of the candidate models, the best $AdjR^2$ and `CrossValidated` pair was found by removing the large leverages after fitting model eight and refitting without those leveraged datapoints.

```{r}

model_ten = lm(KneeScore_Patient ~ . - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = new_dataframe) # Best model so far
evaluate(model_ten)
```

```{r}
evaluate(model_back_bic)
```

```{r}
anova(model_ten, model_back_bic)
```

Of the many models attempted none of the step functions produced the best model.  

## Appendix
### Knee Score Patient
```{r}
model = lm(KneeScore_Patient ~. , data = knee)
model_back_aic = step(model, direction = "backward", trace = 0)
model_for_aic = step(model, direction = "forward", trace = 0)
model_both_aic = step(model, direction = "both", trace = 0)

n = length(resid(model))
model_back_bic = step(model, direction = "backward", k = log(n), trace = 0)
model_for_bic = step(model, direction = "forward", k = log(n), trace = 0)
model_both_bic = step(model, direction = "both", k = log(n), trace = 0)
```

```{r}
evaluate(model)
```

```{r}
evaluate(model_back_aic)
```

```{r}
evaluate(model_for_aic)
```

```{r}
evaluate(model_both_aic)
```

```{r}
## Adj.R.Squared: 0.24 - CrossValidation: 16.9
evaluate(model_back_bic)
```

```{r}
evaluate(model_for_bic)
```

```{r}
evaluate(model_both_bic)
```

```{r}
## Remove large leverages
## Remove large leverages
new_data = removeInfluential(model, knee)
model_without_large_leverages = lm(KneeScore_Patient ~ ., data = new_data)
evaluate(model_without_large_leverages)
```

Removing the records with large hat values and refitting simply introduces more large hat values.  What is unique about these rows of data?

```{r}
which(hatvalues(model) == 1.0)
print(knee[17,])
print(knee[240,])
print(knee[638,])
print(knee[1093,])
print(knee[1775, ])
```

Each of these has a suspect factor entry.  Perhaps the entry only occurs infrequently in the database and is causing the entire row to be flagged as high leverage?
Try pulling the suspect columns out of the LM and see what happens.

```{r}
model_eight = lm(KneeScore_Patient ~ . - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - TibialTraySize - FemoralComponentType, data = knee)
evaluate(model_eight)
```

```{r}
model_nine = lm(KneeScore_Patient ~ . - TibialInsertType - FemoralComponentModel - PatellaModel - TibialTrayModel - FemoralComponentType, data = knee)
evaluate(model_nine)
```

```{r}
sample_idx = sample(1:nrow(knee), 1000)
train_data = knee[sample_idx, ]
test_data = knee[-sample_idx, ]


rmse  = function(actual, predicted) {  sqrt(mean((actual - predicted) ^ 2))}
rmse(test_data$KneeScore_Patient, predict(model_ten, test_data))
rmse(train_data$KneeScore_Patient, predict(model_ten, train_data))

rmse(test_data$KneeScore_Patient, predict(model_back_bic, test_data))
rmse(train_data$KneeScore_Patient, predict(model_back_bic, train_data))
```

```{r}
plot(model_back_bic)
```



